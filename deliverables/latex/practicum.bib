
@article{pinciroli_vago_predicting_2024,
	title = {Predicting {Machine} {Failures} from {Multivariate} {Time} {Series}: {An} {Industrial} {Case} {Study}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2075-1702},
	shorttitle = {Predicting {Machine} {Failures} from {Multivariate} {Time} {Series}},
	url = {https://www.mdpi.com/2075-1702/12/6/357},
	doi = {10.3390/machines12060357},
	abstract = {Non-neural machine learning (ML) and deep learning (DL) are used to predict system failures in industrial maintenance. However, only a few studies have assessed the effect of varying the amount of past data used to make a prediction and the extension in the future of the forecast. This study evaluates the impact of the size of the reading window and of the prediction window on the performances of models trained to forecast failures in three datasets of (1) an industrial wrapping machine working in discrete sessions, (2) an industrial blood refrigerator working continuously, and (3) a nitrogen generator working continuously. A binary classification task assigns the positive label to the prediction window based on the probability of a failure to occur in such an interval. Six algorithms (logistic regression, random forest, support vector machine, LSTM, ConvLSTM, and Transformers) are compared on multivariate time series. The dimension of the prediction windows plays a crucial role and the results highlight the effectiveness of DL approaches in classifying data with diverse time-dependent patterns preceding a failure and the effectiveness of ML approaches in classifying similar and repetitive patterns preceding a failure.},
	language = {en},
	number = {6},
	urldate = {2024-05-25},
	journal = {Machines},
	author = {Pinciroli Vago, Nicol√≤ Oreste and Forbicini, Francesca and Fraternali, Piero},
	month = may,
	year = {2024},
	pages = {357},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	language = {en},
	number = {7587},
	urldate = {2024-05-25},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	pages = {484--489},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	language = {en},
	number = {7676},
	urldate = {2024-05-25},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Van Den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	pages = {354--359},
}

@book{sutton_reinforcement_1998,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-19398-6},
	shorttitle = {Reinforcement learning},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {1998},
	keywords = {Reinforcement learning},
}

@misc{guthrie_nistsematech_2020,
	title = {{NIST}/{SEMATECH} e-{Handbook} of {Statistical} {Methods} ({NIST} {Handbook} 151)},
	copyright = {License Information for NIST data},
	url = {https://www.itl.nist.gov/div898/handbook/},
	abstract = {The NIST/SEMATECH e-Handbook of Statistical Methods is a Web-based book written to help scientists and engineers incorporate statistical methods into their work as efficiently as possible. Ideally, it will serve as a reference which will help scientists and engineers design their own experiments and carry out the appropriate analyses when a statistician is not available to help. It is also hoped that it will serve as a useful educational tool that will help users of statistical methods and consumers of statistical information better understand statistical procedures and their underlying assumptions, and more clearly interpret scientific and engineering results stated in statistical terms.},
	language = {en},
	urldate = {2024-05-25},
	publisher = {National Institute of Standards and Technology},
	author = {Guthrie, William F.},
	year = {2020},
	doi = {10.18434/M32189},
	keywords = {analysis of variance, experimental design, exploratory data analysis, FOS: Mathematics, graphics, model validation, process characterization, quality control, regression, reliability, statistical modeling, Statistics, time series, uncertaintyanalysis},
}

@misc{gao_deep_2018,
	title = {Deep reinforcement learning for time series: playing idealized trading games},
	shorttitle = {Deep reinforcement learning for time series},
	url = {http://arxiv.org/abs/1803.03916},
	abstract = {Deep Q-learning is investigated as an end-to-end solution to estimate the optimal strategies for acting on time series input. Experiments are conducted on two idealized trading games. 1) Univariate: the only input is a wave-like price time series, and 2) Bivariate: the input includes a random stepwise price time series and a noisy signal time series, which is positively correlated with future price changes. The Univariate game tests whether the agent can capture the underlying dynamics, and the Bivariate game tests whether the agent can utilize the hidden relation among the inputs. Stacked Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM) units, Convolutional Neural Network (CNN), and multi-layer perceptron (MLP) are used to model Q values. For both games, all agents successfully find a profitable strategy. The GRU-based agents show best overall performance in the Univariate game, while the MLP-based agents outperform others in the Bivariate game.},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Gao, Xiang},
	month = mar,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv:1803.03916 [cs, stat]},
}

@misc{shaik_graph-enabled_2024,
	title = {Graph-enabled {Reinforcement} {Learning} for {Time} {Series} {Forecasting} with {Adaptive} {Intelligence}},
	url = {http://arxiv.org/abs/2309.10186},
	abstract = {Reinforcement learning is well known for its ability to model sequential tasks and learn latent data patterns adaptively. Deep learning models have been widely explored and adopted in regression and classification tasks. However, deep learning has its limitations such as the assumption of equally spaced and ordered data, and the lack of ability to incorporate graph structure in terms of time-series prediction. Graphical neural network (GNN) has the ability to overcome these challenges and capture the temporal dependencies in time-series data. In this study, we propose a novel approach for predicting time-series data using GNN and monitoring with Reinforcement Learning (RL). GNNs are able to explicitly incorporate the graph structure of the data into the model, allowing them to capture temporal dependencies in a more natural way. This approach allows for more accurate predictions in complex temporal structures, such as those found in healthcare, traffic and weather forecasting. We also fine-tune our GraphRL model using a Bayesian optimisation technique to further improve performance. The proposed framework outperforms the baseline models in time-series forecasting and monitoring. The contributions of this study include the introduction of a novel GraphRL framework for time-series prediction and the demonstration of the effectiveness of GNNs in comparison to traditional deep learning models such as RNNs and LSTMs. Overall, this study demonstrates the potential of GraphRL in providing accurate and efficient predictions in dynamic RL environments.},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Shaik, Thanveer and Tao, Xiaohui and Xie, Haoran and Li, Lin and Yong, Jianming and Li, Yuefeng},
	month = feb,
	year = {2024},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {arXiv:2309.10186 [cs]},
}

@misc{wu_autoformer_2022,
	title = {Autoformer: {Decomposition} {Transformers} with {Auto}-{Correlation} for {Long}-{Term} {Series} {Forecasting}},
	shorttitle = {Autoformer},
	url = {http://arxiv.org/abs/2106.13008},
	abstract = {Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38\% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease. Code is available at this repository: {\textbackslash}textbackslashurl\{https://github.com/thuml/Autoformer\}.},
	urldate = {2024-07-07},
	publisher = {arXiv},
	author = {Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
	month = jan,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {arXiv:2106.13008 [cs]},
}

@misc{chen_decision_2021,
	title = {Decision {Transformer}: {Reinforcement} {Learning} via {Sequence} {Modeling}},
	shorttitle = {Decision {Transformer}},
	url = {http://arxiv.org/abs/2106.01345},
	abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
	urldate = {2024-07-07},
	publisher = {arXiv},
	author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
	month = jun,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {arXiv:2106.01345 [cs]},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-07-07},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	keywords = {Computer Science - Computation and Language},
	annote = {arXiv:1810.04805 [cs]},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2024-07-07},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, J√ºrgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2024-07-07},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K√∂pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Mathematical Software},
	annote = {arXiv:1912.01703 [cs, stat]},
}

@misc{goswami_moment_2024,
	title = {{MOMENT}: {A} {Family} of {Open} {Time}-series {Foundation} {Models}},
	shorttitle = {{MOMENT}},
	url = {http://arxiv.org/abs/2402.03885},
	abstract = {We introduce MOMENT, a family of open-source foundation models for general-purpose time series analysis. Pre-training large models on time series data is challenging due to (1) the absence of a large and cohesive public time series repository, and (2) diverse time series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time series, called the Time series Pile, and systematically tackle time series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time series models. Pre-trained models (AutonLab/MOMENT-1-large) and Time Series Pile (AutonLab/Timeseries-PILE) are available on Huggingface.},
	urldate = {2024-07-07},
	publisher = {arXiv},
	author = {Goswami, Mononito and Szafer, Konrad and Choudhry, Arjun and Cai, Yifu and Li, Shuo and Dubrawski, Artur},
	month = may,
	year = {2024},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {arXiv:2402.03885 [cs]},
}

@book{lawson_introduction_2021,
	address = {Boca Raton London New York},
	edition = {First edition},
	title = {An introduction to acceptance sampling and {SPC} with {R}},
	isbn = {978-0-367-55576-4 978-0-367-56995-2},
	language = {eng},
	publisher = {CRC Press},
	author = {Lawson, John},
	year = {2021},
	annote = {Literaturverzeichnis: Seite 267-274},
	file = {Table of Contents PDF:/home/dalton/Zotero/storage/YFI9HA6P/Lawson - 2021 - An introduction to acceptance sampling and SPC wit.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-07-09},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/home/dalton/Zotero/storage/MFQV9EXF/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/home/dalton/Zotero/storage/4GSNBZ6N/1706.html:text/html},
}

@article{Pong2018TemporalDM,
  title={Temporal Difference Models: Model-Free Deep RL for Model-Based Control},
  author={Vitchyr H. Pong and Shixiang Shane Gu and Murtaza Dalal and Sergey Levine},
  journal={ArXiv},
  year={2018},
  volume={abs/1802.09081},
  url={https://api.semanticscholar.org/CorpusID:3514022}
}

@article{Lillicrap2015ContinuousCW,
  title={Continuous control with deep reinforcement learning},
  author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Manfred Otto Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
  journal={CoRR},
  year={2015},
  volume={abs/1509.02971},
  url={https://api.semanticscholar.org/CorpusID:16326763}
}

@article{Raffel2019ExploringTL,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Colin Raffel and Noam M. Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal={J. Mach. Learn. Res.},
  year={2019},
  volume={21},
  pages={140:1-140:67},
  url={https://api.semanticscholar.org/CorpusID:204838007}
}

@article{Kingma2014AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2014},
  volume={abs/1412.6980},
  url={https://api.semanticscholar.org/CorpusID:6628106}
}

@article{Loshchilov2016SGDRSG,
  title={SGDR: Stochastic Gradient Descent with Restarts},
  author={Ilya Loshchilov and Frank Hutter},
  journal={ArXiv},
  year={2016},
  volume={abs/1608.03983},
  url={https://api.semanticscholar.org/CorpusID:15884797}
}

@article{Schulman2017ProximalPO,
  title={Proximal Policy Optimization Algorithms},
  author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
  journal={ArXiv},
  year={2017},
  volume={abs/1707.06347},
  url={https://api.semanticscholar.org/CorpusID:28695052}
}

@article{Schulman2015TrustRP,
  title={Trust Region Policy Optimization},
  author={John Schulman and Sergey Levine and P. Abbeel and Michael I. Jordan and Philipp Moritz},
  journal={ArXiv},
  year={2015},
  volume={abs/1502.05477},
  url={https://api.semanticscholar.org/CorpusID:16046818}
}

@article{Berner2019Dota2W,
  title={Dota 2 with Large Scale Deep Reinforcement Learning},
  author={Christopher Berner and Greg Brockman and Brooke Chan and Vicki Cheung and Przemyslaw Debiak and Christy Dennison and David Farhi and Quirin Fischer and Shariq Hashme and Christopher Hesse and Rafal J{\'o}zefowicz and Scott Gray and Catherine Olsson and Jakub W. Pachocki and Michael Petrov and Henrique Pond{\'e} de Oliveira Pinto and Jonathan Raiman and Tim Salimans and Jeremy Schlatter and Jonas Schneider and Szymon Sidor and Ilya Sutskever and Jie Tang and Filip Wolski and Susan Zhang},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.06680},
  url={https://api.semanticscholar.org/CorpusID:209376771}
} 

@article{Hu2021LoRALA,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={J. Edward Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.09685},
  url={https://api.semanticscholar.org/CorpusID:235458009}
}

@misc{meng2024pissaprincipalsingularvalues,
      title={PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models}, 
      author={Fanxu Meng and Zhaohui Wang and Muhan Zhang},
      year={2024},
      eprint={2404.02948},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.02948}, 
}
